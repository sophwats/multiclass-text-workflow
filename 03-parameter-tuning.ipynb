{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models we have trained have parameters which must be tuned. It's common practice to train models using a few different parameter values, and see how they impact a model's performance. \n",
    "\n",
    "It can also be necessary to retune your model's parameters after your model has been running in production for a while - perhaps the data has drifted, and as such the model should be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we load in the feature engineering and model training pipeline stages developed in the previous notebooks, and implement a parameter sweep to identify the best model parameters from a candidate set. \n",
    "\n",
    "We start by loaing in the training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os.path\n",
    "\n",
    "training_data = pd.read_parquet(os.path.join(\"data\", \"training.parquet\"))\n",
    "testing_data = pd.read_parquet(os.path.join(\"data\", \"testing.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the feature engineering and model pipeline stages which were developed in the previous notebooks. We will then combine them into one pipeline, which takes in raw data and returns a prediction.\n",
    "\n",
    "Note: If you didn't run atleast one feature engineering notebook and one model training notebook fully, this next cell will return an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading in feature extraction pipeline\n",
    "import cloudpickle as cp\n",
    "feature_pipeline = cp.load(open('feature_pipeline.sav', 'rb'))\n",
    "\n",
    "model = cp.load(open('model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('features',feature_pipeline),\n",
    "    ('model',model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline can be fit to data (in the same way that we fit the individual feature engineering and model training techniques to the data in the previous notebooks). We can also evaluate the model using the test set, as we did previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(training_data[\"Text\"], training_data[\"Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import plot\n",
    "df, chart = plot.confusion_matrix(testing_data.Category, pipeline.predict(testing_data[\"Text\"]))\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(testing_data.Category, pipeline.predict(testing_data[\"Text\"]) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily retrain the pipeline, using different values for the parameters. `pipeline.named_steps` states the steps in the pipeline which we can refer to by name. We will then use if/else statements to select a parameter grid to sweep over for the different types of models.\n",
    "\n",
    "\n",
    "✅ The parameter sweep below only supports three of the four models you could train in the previous notebooks. Add support for the XGBoost model below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {}\n",
    "\n",
    "if 'MultinomialNB' in str(pipeline.named_steps['model']):\n",
    "    # we trained the naive Bayes model. \n",
    "    print(\"Parameter sweep for the Multinomial Naive Bayes Model\")\n",
    "    param_grid = { 'model__alpha' : [0.1,0.25,0.5,0.75,1] }\n",
    "    print(param_grid)\n",
    "elif 'LinearSVC' in str(pipeline.named_steps['model']):\n",
    "    # We trained the Support vector classifier. \n",
    "    print(\"Parameter sweep for the Linear Support Vector Classifier\")\n",
    "    param_grid = {'model__multi_class' : ['ovr', 'crammer_singer'], \n",
    "                  'model__C': [0.3, 0.6, 1], \n",
    "                  'model__max_iter': [20000]}\n",
    "elif 'RandomForestClassifier' in str(pipeline.named_steps['model']):\n",
    "    print(\"Prameter sweep for the Random Forest Classifier\")\n",
    "    param_grid = {'model__max_depth': [3, 4, 5, 6], \n",
    "                  'model__n_estimators': [100, 250, 500]}\n",
    "else:\n",
    "    # we haven't dealt with this model yet \n",
    "    print(\"Parameter grid not defined for this model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "search = None\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid, cv=3, return_train_score=True)\n",
    "search.fit(training_data[\"Text\"], training_data[\"Category\"])\n",
    "\n",
    "print(\"Best parameters were %s\" % str(search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `.cv_results` to see more information about the training performance at each of the candidate sets of parameter values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
